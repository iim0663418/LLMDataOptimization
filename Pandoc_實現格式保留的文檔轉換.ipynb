{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwG/5SyFVojjfhKUqaR2ra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iim0663418/LLMDataOptimization/blob/main/Pandoc_%E5%AF%A6%E7%8F%BE%E6%A0%BC%E5%BC%8F%E4%BF%9D%E7%95%99%E7%9A%84%E6%96%87%E6%AA%94%E8%BD%89%E6%8F%9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandoc 是一個通用的文檔轉換工具，可以在多種格式之間轉換文檔，如 DOCX、ODT、Markdown、HTML、LaTeX 等。它能夠保留文檔的格式，並提供多種輸出選項。\n",
        "這段程式碼只支援 DOCX、ODT 運用：\n",
        "1. 先將文檔轉換為 HTML 格式，保留格式資訊。\n",
        "2. 然後使用自定義的解析器從 HTML 中提取所需的結構化數據，並進一步轉換為其他格式。</br>\n",
        "\n",
        "方法論轉換成純文字檔案的過程。\n",
        "\n",
        "同時加入將 json 轉換成語言模型友善的 .txt 方法，藉此保留原有的格式"
      ],
      "metadata": {
        "id": "WzwWdghumUNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K67Teg75jYnl",
        "outputId": "9b9209b1-b132-45d5-8b47-de2a05a31d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc-data\n",
            "Suggested packages:\n",
            "  texlive-latex-recommended texlive-xetex texlive-luatex pandoc-citeproc\n",
            "  texlive-latex-extra context wkhtmltopdf librsvg2-bin groff ghc nodejs php\n",
            "  python ruby libjs-mathjax libjs-katex citation-style-language-styles\n",
            "The following NEW packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc\n",
            "  pandoc-data\n",
            "0 upgraded, 4 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 20.6 MB of archives.\n",
            "After this operation, 156 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Fetched 20.6 MB in 1s (35.2 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "(Reading database ... 123595 files and directories currently installed.)\n",
            "Preparing to unpack .../libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install pandoc\n",
        "!pip install beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def create_directories():\n",
        "    input_dir = \"/content/input\"\n",
        "    output_dir = \"/content/output\"\n",
        "    txtOutput_dir = \"/content/output/txtOutput\"\n",
        "    log_dir = \"/content/log\"\n",
        "\n",
        "    # 創建 /content/input 目錄\n",
        "    os.makedirs(input_dir, exist_ok=True)\n",
        "    print(f\"Directory created: {input_dir}\")\n",
        "\n",
        "    # 創建 /content/output 目錄\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"Directory created: {output_dir}\")\n",
        "    # 創建 /content/output/txtOutput 目錄\n",
        "    os.makedirs(txtOutput_dir, exist_ok=True)\n",
        "    print(f\"Directory created: {txtOutput_dir}\")\n",
        "    # 創建 /content/log 目錄\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    print(f\"Directory created: {log_dir}\")\n",
        "# 執行函數創建目錄\n",
        "create_directories()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao1gyAw_lm75",
        "outputId": "c9618888-11bf-4007-9725-a2e5876a5519"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory created: /content/input\n",
            "Directory created: /content/output\n",
            "Directory created: /content/output/txtOutput\n",
            "Directory created: /content/log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import traceback\n",
        "\n",
        "def convert_to_html(input_file, output_file):\n",
        "    try:\n",
        "        subprocess.run(['pandoc', '-o', output_file, input_file], check=True)\n",
        "        print(f\"Converted {input_file} to {output_file}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error converting {input_file}: {e}\")\n",
        "        raise\n",
        "\n",
        "def parse_html_and_extract_data(html_file):\n",
        "    with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
        "        contents = file.read()\n",
        "        soup = BeautifulSoup(contents, \"html.parser\")\n",
        "\n",
        "    content = []\n",
        "    current_title = None\n",
        "    current_paragraphs = []\n",
        "\n",
        "    title_pattern = re.compile('^h[1-6]$')  # 預編正則表示式提高效率\n",
        "\n",
        "    for element in soup.find_all(lambda tag: re.match(title_pattern, tag.name) or tag.name == 'p'):\n",
        "        if re.match(title_pattern, element.name):  # 使用 re.match\n",
        "            if current_title is not None:\n",
        "                content.append({\n",
        "                    'title': current_title,\n",
        "                    'paragraphs': current_paragraphs\n",
        "                })\n",
        "            current_title = element.text.strip()\n",
        "            current_paragraphs = []\n",
        "        else:\n",
        "            current_paragraphs.append(element.text.strip())\n",
        "\n",
        "    if current_title is not None:\n",
        "        content.append({\n",
        "            'title': current_title,\n",
        "            'paragraphs': current_paragraphs\n",
        "        })\n",
        "\n",
        "    return {'content': content}\n",
        "\n",
        "\n",
        "def save_data_as_json(data, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "    print(f\"Saved JSON to {output_file}\")\n",
        "\n",
        "def batch_process_documents(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    summary_data = []\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.lower().endswith(('.docx', '.odt')):\n",
        "            base_name = os.path.splitext(filename)[0]\n",
        "            input_file = os.path.join(input_folder, filename)\n",
        "            html_file = os.path.join(output_folder, f\"{base_name}.html\")\n",
        "            json_file = os.path.join(output_folder, f\"{base_name}.json\")\n",
        "\n",
        "            try:\n",
        "                convert_to_html(input_file, html_file)\n",
        "                data = parse_html_and_extract_data(html_file)\n",
        "                save_data_as_json(data, json_file)\n",
        "                summary_data.append({base_name: data})\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "    summary_file = os.path.join(output_folder, \"summary.json\")\n",
        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(summary_data, f, indent=4)\n",
        "    print(f\"Summary saved to {summary_file}\")\n",
        "\n",
        "# 使用範例\n",
        "input_folder = \"/content/input\"\n",
        "output_folder = \"/content/output\"\n",
        "batch_process_documents(input_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "HEiVTTsvjn9x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e59e44-f5f7-4d1d-d7cd-0a99d71ab84a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted /content/input/AI_Risk_Assessment_Database_Interpretation_Document.docx to /content/output/AI_Risk_Assessment_Database_Interpretation_Document.html\n",
            "Saved JSON to /content/output/AI_Risk_Assessment_Database_Interpretation_Document.json\n",
            "Summary saved to /content/output/summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def json_to_txt(data, output_file):\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for item in data:\n",
        "                f.write(f\"## 文件來源: {item['filename']}\\n\\n\")\n",
        "\n",
        "                for content in item['content']:\n",
        "                    if 'title' in content:\n",
        "                        f.write(f\"### 標題: {content['title']}\\n\")\n",
        "                    if 'paragraphs' in content:\n",
        "                        for paragraph in content['paragraphs']:\n",
        "                            f.write(f\"{paragraph}\\n\\n\")\n",
        "                    if 'lists' in content:\n",
        "                        for lst in content['lists']:\n",
        "                            f.write(\"列表項目:\\n\")\n",
        "                            for list_item in lst['items']:\n",
        "                                f.write(f\"- {list_item}\\n\")\n",
        "                            f.write(\"\\n\")\n",
        "                    if 'tables' in content:\n",
        "                        for table in content['tables']:\n",
        "                            f.write(\"表格內容:\\n\")\n",
        "                            for row in table['rows']:\n",
        "                                f.write(\" | \".join(row) + \"\\n\")\n",
        "                            f.write(\"\\n\")\n",
        "\n",
        "        print(f\"Converted JSON data to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to write to {output_file}: {e}\")\n",
        "\n",
        "def batch_json_to_single_txt(input_folder, output_txt_file, output_json_file):\n",
        "    combined_data = []\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith('.json'):\n",
        "            json_file = os.path.join(input_folder, filename)\n",
        "            try:\n",
        "                with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                combined_data.append({\n",
        "                    \"filename\": filename,\n",
        "                    \"content\": data['content']  # Assuming data structure is adjusted as per new parse method\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to process {json_file}: {e}\")\n",
        "\n",
        "    json_to_txt(combined_data, output_txt_file)\n",
        "\n",
        "    try:\n",
        "        with open(output_json_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(combined_data, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Saved combined data to {output_json_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to write combined data to {output_json_file}: {e}\")\n",
        "\n",
        "# 使用範例\n",
        "input_folder = \"/content/output\"\n",
        "output_txt_file = \"/content/output/txtOutput/combined_output.txt\"\n",
        "output_json_file = \"/content/output/txtOutput/combined_output.json\"\n",
        "\n",
        "batch_json_to_single_txt(input_folder, output_txt_file, output_json_file)\n"
      ],
      "metadata": {
        "id": "Y9cVZvglwiej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab9d67d-5a81-4bb0-eb0e-4f1ca9df2fc7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to process /content/output/summary.json: list indices must be integers or slices, not str\n",
            "Converted JSON data to /content/output/txtOutput/combined_output.txt\n",
            "Saved combined data to /content/output/txtOutput/combined_output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_output_directory():\n",
        "    output_dir = \"/content/output\"\n",
        "    zip_file_path = \"/content/output_archive.zip\"\n",
        "\n",
        "    # 確保 output 目錄存在\n",
        "    if os.path.exists(output_dir):\n",
        "        # 使用 shutil.make_archive 創建 ZIP 文件\n",
        "        shutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', output_dir)\n",
        "        print(f\"Directory {output_dir} has been zipped into {zip_file_path}\")\n",
        "    else:\n",
        "        print(f\"Directory {output_dir} does not exist\")\n",
        "\n",
        "# 執行打包\n",
        "zip_output_directory()\n"
      ],
      "metadata": {
        "id": "ZUGoxQo9nMHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4f2972-9d25-4464-c4e8-730a884af639"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory /content/output has been zipped into /content/output_archive.zip\n"
          ]
        }
      ]
    }
  ]
}